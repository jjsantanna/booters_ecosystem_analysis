{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cfscrape\n",
    "from lxml import etree\n",
    "import datetime\n",
    "import os.path\n",
    "\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://productforums.google.com/forum/#!msg/webmasters/8fPg1I2p34w/Xsdw0stkDwAJ and https://moz.com/blog/the-ultimate-guide-to-the-google-search-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "booter\n",
      "Page number:  1\n",
      "Page number:  2\n",
      "Page number:  3\n",
      "Page number:  4\n",
      "Page number:  5\n",
      "Page number:  6\n",
      "Page number:  7\n",
      "Page number:  8\n",
      "Page number:  9\n",
      "Page number:  10\n",
      "Page number:  11\n",
      "Page number:  12\n",
      "Page number:  13\n",
      "Page number:  14\n",
      "Page number:  15\n",
      "Page number:  16\n",
      "Page number:  17\n",
      "Page number:  18\n",
      "Page number:  19\n",
      "Page number:  20\n",
      "Page number:  21\n",
      "Page number:  22\n",
      "Page number:  23\n",
      "Page number:  24\n",
      "Page number:  25\n",
      "\t*Empty\n",
      "Page number:  26\n",
      "\t*Empty\n",
      "Page number:  27\n",
      "Page number:  28\n",
      "\t*Empty\n",
      "stresser\n",
      "Page number:  1\n",
      "Page number:  2\n",
      "Page number:  3\n",
      "Page number:  4\n",
      "Page number:  5\n",
      "Page number:  6\n",
      "Page number:  7\n",
      "Page number:  8\n",
      "Page number:  9\n",
      "Page number:  10\n",
      "Page number:  11\n",
      "Page number:  12\n",
      "Page number:  13\n",
      "Page number:  14\n",
      "Page number:  15\n",
      "\t*Empty\n",
      "Page number:  16\n",
      "\t*Empty\n",
      "Page number:  17\n",
      "\t*Empty\n",
      "ddoser\n",
      "Page number:  1\n",
      "Page number:  2\n",
      "Page number:  3\n",
      "Page number:  4\n",
      "Page number:  5\n",
      "Page number:  6\n",
      "Page number:  7\n",
      "Page number:  8\n",
      "Page number:  9\n",
      "Page number:  10\n",
      "Page number:  11\n",
      "Page number:  12\n",
      "Page number:  13\n",
      "Page number:  14\n",
      "Page number:  15\n",
      "Page number:  16\n",
      "Page number:  17\n",
      "Page number:  18\n",
      "Page number:  19\n",
      "Page number:  20\n",
      "Page number:  21\n",
      "\t*Empty\n",
      "Page number:  22\n",
      "\t*Empty\n",
      "Page number:  23\n",
      "\t*Empty\n"
     ]
    }
   ],
   "source": [
    "df_urls=pd.DataFrame()\n",
    "\n",
    "keywords = ['booter','stresser','ddoser']\n",
    "\n",
    "for keyword in keywords:\n",
    "    print(keyword)\n",
    "    \n",
    "    page_number=1\n",
    "    attempts=0\n",
    "    while attempts < 3:\n",
    "        print('Page number: ',str(page_number))\n",
    "\n",
    "        url=\"https://www.google.nl/search?q=\"+keyword+\"&start=\"+str((page_number - 1)*10)\n",
    "        scraper = cfscrape.create_scraper()\n",
    "        scraped_html=scraper.get(url).content\n",
    "        html_tree = etree.HTML(scraped_html)\n",
    "\n",
    "        results_temp=html_tree.xpath(\"//h3/a/@href\") #for the links\n",
    "\n",
    "        if not results_temp:       \n",
    "            attempts += 1\n",
    "            print('\\t*Empty')\n",
    "\n",
    "        df_urls=df_urls.append(pd.DataFrame(results_temp, [keyword]*len(results_temp)))\n",
    "\n",
    "        page_number+= 1\n",
    "        sleep(randint(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "575"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_urls.reset_index(inplace=True)\n",
    "len(df_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STRIP http(s) and www and leave only the domain name + TLD\n",
    "df_urls['strip_url']=df_urls[0].str.split('//').str[1].\\\n",
    "                                str.split('/').str[0].\\\n",
    "                                str.replace('www.','')\n",
    "        \n",
    "# DROP duplicated \n",
    "df_remaining=df_urls.drop_duplicates(subset=['strip_url']).dropna()\n",
    "len(df_remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "not_booter=['wikipedia',\n",
    "            'webopedia',\n",
    "            'wiktionary',\n",
    "            'merriam-webster',\n",
    "            'twitter',\n",
    "            'reddit',\n",
    "            'incapsula',\n",
    "            'github', \n",
    "            'youtube',\n",
    "            '.google',\n",
    "            'urbandictionary',\n",
    "            'arstechnica',\n",
    "            'sourceforge',\n",
    "            'aliexpress',\n",
    "            'instagram',\n",
    "            'arbornetworks',\n",
    "            'radware',\n",
    "            'damonmccoy',\n",
    "            'jairsantanna',\n",
    "            'krebsonsecurity',\n",
    "            'booterblacklist.com',\n",
    "            'hackforum',\n",
    "            'facebook',\n",
    "            'amazon',\n",
    "            'tripadvisor',\n",
    "            'linkedin',\n",
    "            'spotify',\n",
    "            'linguee',\n",
    "            'glassdoor',\n",
    "            'forum.',\n",
    "            'blog.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_remaining=df_remaining[~df_remaining['strip_url'].str.contains('|'.join(not_booter))]['strip_url']\n",
    "len(df_remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1035"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_groundtruth = pd.read_csv('20180215_groundtruth.csv')\n",
    "len(df_groundtruth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a file of urls '2classify' (manually or automatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2classify=df_remaining[~df_remaining.isin(df_groundtruth['url'])]\n",
    "df_2classify.to_csv('2classify.csv',  index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After classifying URLs, then merge with the current ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20180215'"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "today=f\"{datetime.datetime.now():%Y%m%d}\"\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_groundtruth=df_groundtruth.append(pd.read_csv ('2classify.csv',sep = ';'))\n",
    "df_groundtruth.drop_duplicates().to_csv(today+'_groundtruth.csv', index = False)\n",
    "\n",
    "!rm 2classify.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1035"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_groundtruth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the booterlist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOT NEEDED!!\n",
    "df_groundtruth=pd.read_csv(today+'_groundtruth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "booterlist = df_groundtruth[df_groundtruth['booter?']== 'Y']['url'].drop_duplicates().sort_values()\n",
    "booterlist.to_csv(today+'_booterlist.csv',  index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
